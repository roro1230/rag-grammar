{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edbc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231494fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 137 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41 pages; removed unwanted boilerplate where found.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "# Load PDF and remove a known unwanted boilerplate sentence on load\n",
    "loader = PyPDFLoader(\"INTENSIVE GRAMMAR.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Exact sentence to remove (leading newline optional). Case-insensitive match.\n",
    "target = \"M·ªçi thng tin v·ªÅ kho√° h·ªçc vui l√≤ng li√™n h·ªá Zalo official: The Forum Education ‚Äì c√≥ tick xanh t·∫°i √¥ search\"\n",
    "pattern = re.compile(r\"\\n?\" + re.escape(target), flags=re.IGNORECASE)\n",
    "\n",
    "cleaned_docs = []\n",
    "for d in docs:\n",
    "    text = getattr(d, 'page_content', str(d))\n",
    "    # remove the unwanted sentence if present\n",
    "    new_text = pattern.sub('', text)\n",
    "    # normalize excessive blank lines\n",
    "    new_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", new_text)\n",
    "    if new_text != text:\n",
    "        meta = d.metadata if hasattr(d, 'metadata') else {}\n",
    "        cleaned_docs.append(Document(page_content=new_text, metadata=meta))\n",
    "    else:\n",
    "        cleaned_docs.append(d)\n",
    "\n",
    "docs = cleaned_docs\n",
    "print(f\"Loaded {len(docs)} pages; removed unwanted boilerplate where found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a3021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 41\n",
      "       1  \n",
      "PH·∫¶N 1: ƒê·ªòNG T·ª™ V√Ä TH√å (VERB TENSES) 1.1. T·ªïng quan v·ªÅ th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh Th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh th·ªÉ hi·ªán th·ªùi gian v√† tr·∫°ng th√°i c·ªßa h√†nh ƒë·ªông ho·∫∑c tr·∫°ng th√°i. C√≥ 12 th√¨ c∆° b·∫£n trong ti·∫øng Anh, chia th√†nh ba th·ªùi ch√≠nh: hi·ªán t·∫°i, qu√° kh·ª© v√† t∆∞∆°ng lai, m·ªói th·ªùi c√≥ b·ªën th√¨ nh·ªè (ƒë∆°n, ti·∫øp di·ªÖn, ho√†n th√†nh, ho√†n th√†nh ti·∫øp di·ªÖn). 1.1.1. Th√¨ hi·ªán t·∫°i (Present Tenses) ‚Ä¢ Hi·ªán t·∫°i ƒë∆°n (Present Simple): Di·ªÖn t·∫£ th√≥i quen, s·ª± th·∫≠t hi·ªÉn nhi√™n, l·ªãch tr√¨nh. o C·∫•u tr√∫c:  ¬ß Kh·∫≥ng ƒë·ªãnh: S + V(s/es) ¬ß Ph·ªß ƒë·ªãnh: S + do/does + not + V-nguy√™n th·ªÉ ¬ß Nghi v·∫•n: Do/Does + S + V-nguy√™n th·ªÉ? o V√≠ d·ª•:  ¬ß She works every day. ¬ß They do not play football. ¬ß Does he like coffee? ‚Ä¢ Hi·ªán t·∫°i ti·∫øp di·ªÖn (Present Continuous): Di·ªÖn t·∫£ h√†nh ƒë·ªông ƒëang di·ªÖn ra t·∫°i th·ªùi ƒëi·ªÉm n√≥i ho·∫∑c k·∫ø ho·∫°ch t∆∞∆°ng lai g·∫ßn. o C·∫•u tr√∫c:  ¬ß S + am/is/are + V-ing o V√≠ d·ª•:  ¬ß I am studying now. ¬ß They are meeting us tomorrow. \n",
      "       2  \n",
      "‚Ä¢ Hi·ªán t·∫°i ho√†n th√†nh (Present Perfect): Di·ªÖn t·∫£ h√†nh ƒë·ªông ƒë√£ x·∫£y ra v√† c√≥ k·∫øt qu·∫£ ƒë·∫øn hi·ªán t·∫°i ho·∫∑c tr·∫£i nghi·ªám. o C·∫•u tr√∫c:  ¬ß S + have/has + V3/V-ed o V√≠ d·ª•:  ¬ß She has visited London twice. ¬ß We have finished our work. ‚Ä¢ Hi·ªán t·∫°i ho√†n th√†nh ti·∫øp di·ªÖn (Present Perfect Continuous): Di·ªÖn t·∫£ h√†nh ƒë·ªông b·∫Øt ƒë·∫ßu trong qu√° kh·ª© v√† v·∫´n ti·∫øp t·ª•c ·ªü hi·ªán t·∫°i. o C·∫•u tr√∫c:  ¬ß S + have/has + been + V-ing o V√≠ d·ª•:  ¬ß They have been working here since morning. 1.1.2. Th√¨ qu√° kh·ª© (Past Tenses) ‚Ä¢ Qu√° kh·ª© ƒë∆°n (Past Simple): Di·ªÖn t·∫£ h√†nh ƒë·ªông ƒë√£ x·∫£y ra v√† k·∫øt th√∫c trong qu√° kh·ª©. o C·∫•u tr√∫c:  ¬ß Kh·∫≥ng ƒë·ªãnh: S + V2/V-ed ¬ß Ph·ªß ƒë·ªãnh: S + did not + V-nguy√™n th·ªÉ ¬ß Nghi v·∫•n: Did + S + V-nguy√™n th·ªÉ? o V√≠ d·ª•:  ¬ß He went to the store yesterday. \n"
     ]
    }
   ],
   "source": [
    "# Inspect the loaded documents from the PDF\n",
    "print(\"Total pages loaded:\", len(docs))\n",
    "print(docs[0].page_content[:1000])\n",
    "print(docs[1].page_content[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7038428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for text chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388ef1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs loaded: 41\n",
      "Total chunks produced: 54\n",
      "\n",
      "--- Chunk 0 (source=INTENSIVE GRAMMAR.pdf) ---\n",
      "1   PH·∫¶N 1: ƒê·ªòNG T·ª™ V√Ä TH√å (VERB TENSES) 1.1. T·ªïng quan v·ªÅ th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh Th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh th·ªÉ hi·ªán th·ªùi gian v√† tr·∫°ng th√°i c·ªßa h√†nh ƒë·ªông ho·∫∑c tr·∫°ng th√°i. C√≥ 12 th√¨ c∆° b·∫£n trong ti·∫øng Anh, chia th√†nh ba th·ªùi ch√≠nh: hi·ªán t·∫°i, qu√° kh·ª© v√† t∆∞∆°ng lai, m·ªói th·ªùi c√≥ b·ªën th√¨ nh·ªè (ƒë∆°n, ti·∫øp di·ªÖn, ho√†n th√†nh, ho√†n th√†nh ti·∫øp di·ªÖn). 1.1.1. Th√¨ hi·ªán t·∫°i (Present Tenses) ‚Ä¢ Hi·ªán t·∫°i ƒë∆°n (Present Simple): Di·ªÖn t·∫£ th√≥i quen, s·ª± th·∫≠t hi·ªÉn nhi√™n, l·ªãch tr√¨nh. o C·∫•u tr√∫c:  ¬ß Kh·∫≥ng ƒë·ªãnh: S + V\n",
      "\n",
      "--- Chunk 1 (source=INTENSIVE GRAMMAR.pdf) ---\n",
      "2   ‚Ä¢ Hi·ªán t·∫°i ho√†n th√†nh (Present Perfect): Di·ªÖn t·∫£ h√†nh ƒë·ªông ƒë√£ x·∫£y ra v√† c√≥ k·∫øt qu·∫£ ƒë·∫øn hi·ªán t·∫°i ho·∫∑c tr·∫£i nghi·ªám. o C·∫•u tr√∫c:  ¬ß S + have/has + V3/V-ed o V√≠ d·ª•:  ¬ß She has visited London twice. ¬ß We have finished our work. ‚Ä¢ Hi·ªán t·∫°i ho√†n th√†nh ti·∫øp di·ªÖn (Present Perfect Continuous): Di·ªÖn t·∫£ h√†nh ƒë·ªông b·∫Øt ƒë·∫ßu trong qu√° kh·ª© v√† v·∫´n ti·∫øp t·ª•c ·ªü hi·ªán t·∫°i. o C·∫•u tr√∫c:  ¬ß S + have/has + been + V-ing o V√≠ d·ª•:  ¬ß They have been working here since morning. 1.1.2. Th√¨ qu√° kh·ª© (Past Tenses) ‚Ä¢ Qu√° kh·ª© ƒë∆°\n",
      "\n",
      "--- Chunk 2 (source=INTENSIVE GRAMMAR.pdf) ---\n",
      "3   ¬ß She did not eat breakfast. ¬ß Did they arrive on time? ‚Ä¢ Qu√° kh·ª© ti·∫øp di·ªÖn (Past Continuous): Di·ªÖn t·∫£ h√†nh ƒë·ªông ƒëang di·ªÖn ra t·∫°i m·ªôt th·ªùi ƒëi·ªÉm c·ª• th·ªÉ trong qu√° kh·ª©. o C·∫•u tr√∫c:  ¬ß S + was/were + V-ing o V√≠ d·ª•:  ¬ß I was reading a book at 8 PM last night. ‚Ä¢ Qu√° kh·ª© ho√†n th√†nh (Past Perfect): Di·ªÖn t·∫£ h√†nh ƒë·ªông x·∫£y ra tr∆∞·ªõc m·ªôt h√†nh ƒë·ªông kh√°c trong qu√° kh·ª©. o C·∫•u tr√∫c:  ¬ß S + had + V3/V-ed o V√≠ d·ª•:  ¬ß When we arrived, they had left. ‚Ä¢ Qu√° kh·ª© ho√†n th√†nh ti·∫øp di·ªÖn (Past Perfect Continuous): Di·ªÖn\n",
      "\n",
      "--- Chunk 3 (source=INTENSIVE GRAMMAR.pdf) ---\n",
      "4   ‚Ä¢ T∆∞∆°ng lai ƒë∆°n (Future Simple): Di·ªÖn t·∫£ d·ª± ƒëo√°n, quy·∫øt ƒë·ªãnh t·∫°i th·ªùi ƒëi·ªÉm n√≥i. o C·∫•u tr√∫c:  ¬ß S + will + V-nguy√™n th·ªÉ o V√≠ d·ª•:  ¬ß She will travel to Japan next month. ‚Ä¢ T∆∞∆°ng lai ti·∫øp di·ªÖn (Future Continuous): Di·ªÖn t·∫£ h√†nh ƒë·ªông s·∫Ω ƒëang di·ªÖn ra t·∫°i m·ªôt th·ªùi ƒëi·ªÉm c·ª• th·ªÉ trong t∆∞∆°ng lai. o C·∫•u tr√∫c:  ¬ß S + will be + V-ing o V√≠ d·ª•:  ¬ß This time next week, I will be lying on the beach. ‚Ä¢ T∆∞∆°ng lai ho√†n th√†nh (Future Perfect): Di·ªÖn t·∫£ h√†nh ƒë·ªông s·∫Ω ho√†n th√†nh tr∆∞·ªõc m·ªôt th·ªùi ƒëi·ªÉm trong t∆∞∆°ng lai. o\n",
      "\n",
      "--- Chunk 4 (source=INTENSIVE GRAMMAR.pdf) ---\n",
      "5\n",
      "Chunk words: min 1 median 165.0 max 229\n",
      "Saved chunks to INTENSIVE_GRAMMAR_chunks.jsonl\n",
      "First saved chunk preview: {\"text\": \"1  \\nPH·∫¶N 1: ƒê·ªòNG T·ª™ V√Ä TH√å (VERB TENSES) 1.1. T·ªïng quan v·ªÅ th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh Th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh th·ªÉ hi·ªán th·ªùi gian v√† tr·∫°ng th√°i c·ªßa h√†nh ƒë·ªông ho·∫∑c tr·∫°ng th√°i. C√≥ 12 th√¨ c∆° b·∫£n trong ti·∫øng Anh, chia th√†nh ba th·ªùi ch√≠nh: hi·ªán t·∫°i, qu√° kh·ª© v√† t∆∞∆°ng lai, m·ªói th·ªùi c√≥ b·ªën th√¨ nh·ªè (ƒë∆°n, ti·∫øp di·ªÖn, ho√†n th√†nh, ho√†n th√†nh ti·∫øp di·ªÖn). 1.1.1. Th√¨ hi·ªán t·∫°i (Present Tenses) ‚Ä¢ Hi·ªán t·∫°i ƒë∆°n (Present Simple): Di·ªÖn t·∫£ th√≥i quen, s·ª± th·∫≠t hi·ªÉn nhi√™n, l·ªãch tr√¨nh. o C·∫•u tr√∫c:  ¬ß Kh·∫≥ng \n"
     ]
    }
   ],
   "source": [
    "# --- Chunk the loaded PDF documents and test examples ---\n",
    "print('Docs loaded:', len(docs))\n",
    "# Use RecursiveCharacterTextSplitter to create chunks sized for embeddings/LLMs\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print('Total chunks produced:', len(chunks))\n",
    "\n",
    "# Show a few sample chunks (preview)\n",
    "for i, c in enumerate(chunks[:5]):\n",
    "    src = c.metadata.get('source') if isinstance(c.metadata, dict) else None\n",
    "    print(f'\\n--- Chunk {i} (source={src}) ---')\n",
    "    print(c.page_content[:500].replace('\\n', ' '))\n",
    "\n",
    "# Basic statistics about chunk sizes (word counts)\n",
    "lengths = [len(c.page_content.split()) for c in chunks]\n",
    "import statistics\n",
    "if lengths:\n",
    "    print('Chunk words: min', min(lengths), 'median', statistics.median(lengths), 'max', max(lengths))\n",
    "\n",
    "# Save chunks to a JSONL file for later indexing (UTF-8)\n",
    "import json\n",
    "out_path = 'INTENSIVE_GRAMMAR_chunks.jsonl'\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    for c in chunks:\n",
    "        meta = c.metadata if hasattr(c, 'metadata') else {}\n",
    "        obj = {'text': c.page_content, 'metadata': meta}\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n",
    "print('Saved chunks to', out_path)\n",
    "\n",
    "# Quick test: load the saved JSONL and print first item to verify\n",
    "with open(out_path, 'r', encoding='utf-8') as f:\n",
    "    first = f.readline()\n",
    "    print('First saved chunk preview:', first[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b2b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Creating embeddings for chunks (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63812/1946595596.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created with 54 chunks\n",
      "‚úÖ Saved vector store to INTENSIVE_GRAMMAR_faiss_index/\n",
      "\n",
      "==================================================\n",
      "Testing similarity search...\n",
      "==================================================\n",
      "\n",
      "üîç Query: 'Th√¨ qu√° kh·ª© ti·∫øp di·ªÖn'\n",
      "  Result 1 (page=7):\n",
      "    1.2.5. Qu√° kh·ª© ti·∫øp di·ªÖn (Past Continuous) L·ªói 1: Kh√¥ng s·ª≠ d·ª•ng qu√° kh·ª© ti·∫øp di·ªÖn ƒë√∫ng ng·ªØ c·∫£nh Sai: When I arrived, they played football. S·ª≠a: When I arrived, they were playing football. Gi·∫£i th√≠ch: H√†nh ƒë·ªông ƒëang di·ªÖn ra trong qu√° kh·ª© (\"they were playing\") khi m·ªôt h√†nh ƒë·ªông kh√°c x·∫£y ƒë·∫øn (\"I arrive...\n",
      "  Result 2 (page=4):\n",
      "    hi·ªán t·∫°i ƒë∆°n cho k·∫ø ho·∫°ch t∆∞∆°ng lai Sai: I go to the dentist tomorrow. S·ª≠a: I am going to the dentist tomorrow. Gi·∫£i th√≠ch: Khi n√≥i v·ªÅ k·∫ø ho·∫°ch t∆∞∆°ng lai ƒë√£ ƒë·ªãnh tr∆∞·ªõc, s·ª≠ d·ª•ng hi·ªán t·∫°i ti·∫øp di·ªÖn....\n",
      "  Result 3 (page=7):\n",
      "    ho√†n th√†nh khi kh√¥ng c·∫ßn thi·∫øt Sai: She had gone to the store yesterday. S·ª≠a: She went to the store yesterday. Gi·∫£i th√≠ch: N·∫øu ch·ªâ c√≥ m·ªôt h√†nh ƒë·ªông trong qu√° kh·ª©, v√† kh√¥ng c√≥ h√†nh ƒë·ªông n√†o kh√°c ƒë·ªÉ so s√°nh th·ªùi gian, th√¨ d√πng qu√° kh·ª© ƒë∆°n. 1.2.7. T∆∞∆°ng lai ƒë∆°n (Future Simple) L·ªói 1: S·ª≠ d·ª•ng \"will\" cho...\n",
      "  Result 4 (page=6):\n",
      "    7   S·ª≠a: They have eaten lunch. Gi·∫£i th√≠ch: ƒê·ªông t·ª´ ph√¢n t·ª´ \"eat\" l√† \"eaten\". C·∫ßn ch√∫ √Ω c√°c ƒë·ªông t·ª´ b·∫•t quy t·∫Øc. L·ªói 3: S·ª≠ d·ª•ng hi·ªán t·∫°i ho√†n th√†nh thay v√¨ hi·ªán t·∫°i ho√†n th√†nh ti·∫øp di·ªÖn Sai: I have lived here for five years (nh·∫•n m·∫°nh qu√° tr√¨nh). S·ª≠a: I have been living here for five years. Gi·∫£i th√≠...\n",
      "  Result 5 (page=0):\n",
      "    1   PH·∫¶N 1: ƒê·ªòNG T·ª™ V√Ä TH√å (VERB TENSES) 1.1. T·ªïng quan v·ªÅ th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh Th√¨ ƒë·ªông t·ª´ trong ti·∫øng Anh th·ªÉ hi·ªán th·ªùi gian v√† tr·∫°ng th√°i c·ªßa h√†nh ƒë·ªông ho·∫∑c tr·∫°ng th√°i. C√≥ 12 th√¨ c∆° b·∫£n trong ti·∫øng Anh, chia th√†nh ba th·ªùi ch√≠nh: hi·ªán t·∫°i, qu√° kh·ª© v√† t∆∞∆°ng lai, m·ªói th·ªùi c√≥ b·ªën th√¨ nh·ªè (ƒë∆°n,...\n",
      "\n",
      "==================================================\n",
      "Search with scores (lower is better)...\n",
      "==================================================\n",
      "  Result 1 (score=1.3505):\n",
      "    30   3. He talked to me ____ he knew me well. ƒê√°p √°n: as if 4. You can have coffee ____ tea. ƒê√°p √°n: or 5. She not only sings ____ also dances well. ƒê√°p √°n: but...\n",
      "  Result 2 (score=1.3796):\n",
      "    C√¢u gh√©p (Compound Sentences) C√¢u gh√©p bao g·ªìm hai ho·∫∑c nhi·ªÅu m·ªánh ƒë·ªÅ ƒë·ªôc l·∫≠p, k·∫øt n·ªëi v·ªõi nhau b·∫±ng li√™n t·ª´ k·∫øt h·ª£p (coordinating conjunction) ho·∫∑c d·∫•u ch·∫•m ph·∫©y. Li√™n t·ª´ k·∫øt h·ª£p (FANBOYS): ‚Ä¢ For (v√¨...\n",
      "  Result 3 (score=1.4020):\n",
      "    12...\n",
      "  Result 4 (score=1.4135):\n",
      "    11   ƒê√°p √°n: 1. has not finished 2. will have started 3. visit 4. have been working 5. was studying / rang B√†i t·∫≠p 2: S·ª≠a l·ªói sai trong c√°c c√¢u sau 1. I am loving this book. 2. They has lived here for...\n",
      "  Result 5 (score=1.4411):\n",
      "    35   4.2.6. ƒê·∫°i t·ª´ (Pronouns) L·ªói 1: S·ª≠ d·ª•ng sai ƒë·∫°i t·ª´ nh√¢n x∆∞ng Sai: Me and him went to the park. S·ª≠a: He and I went to the park. Gi·∫£i th√≠ch: Ch·ªß ng·ªØ c·ªßa c√¢u ph·∫£i ·ªü d·∫°ng ch·ªß c√°ch (I, he). L·ªói 2: Nh·∫ß...\n",
      "\n",
      "‚úÖ FAISS vector store is ready for RAG!\n",
      "   - 54 chunks indexed\n",
      "   - Saved to: INTENSIVE_GRAMMAR_faiss_index/\n",
      "   - Use vector_store.similarity_search(query, k=5) to retrieve top-k chunks\n"
     ]
    }
   ],
   "source": [
    "# --- Create embeddings and FAISS vector store ---\n",
    "print(\"‚è≥ Creating embeddings for chunks (this may take a minute)...\")\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Use a lightweight local embedding model (no API calls)\n",
    "# all-MiniLM-L6-v2: 384-dim embeddings, fast and good quality\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create FAISS vector store from chunks\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "print(f\"‚úÖ Vector store created with {len(chunks)} chunks\")\n",
    "\n",
    "# Save the vector store to disk for reuse\n",
    "vector_store_path = \"INTENSIVE_GRAMMAR_faiss_index\"\n",
    "vector_store.save_local(vector_store_path)\n",
    "print(f\"‚úÖ Saved vector store to {vector_store_path}/\")\n",
    "\n",
    "# --- Test similarity search ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing similarity search...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example queries to test\n",
    "test_queries = [\n",
    "    \"Th√¨ qu√° kh·ª© ti·∫øp di·ªÖn\",\n",
    "\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    # search: returns top-k most similar chunks\n",
    "    results = vector_store.similarity_search(query, k=5)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        content_preview = result.page_content[:300].replace('\\n', ' ')\n",
    "        meta = result.metadata if hasattr(result, 'metadata') else {}\n",
    "        print(f\"  Result {i} (page={meta.get('page')}):\")\n",
    "        print(f\"    {content_preview}...\")\n",
    "\n",
    "# --- Test similarity_search_with_scores ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Search with scores (lower is better)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "query = \"grammar rules\"\n",
    "results_with_scores = vector_store.similarity_search_with_score(query, k=5)\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    content_preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "    print(f\"  Result {i} (score={score:.4f}):\")\n",
    "    print(f\"    {content_preview}...\")\n",
    "\n",
    "print(\"\\n‚úÖ FAISS vector store is ready for RAG!\")\n",
    "print(f\"   - {len(chunks)} chunks indexed\")\n",
    "print(f\"   - Saved to: {vector_store_path}/\")\n",
    "print(\"   - Use vector_store.similarity_search(query, k=5) to retrieve top-k chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c51e8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63812/151724209.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain initialized!\n",
      "\n",
      "======================================================================\n",
      "INTERACTIVE GRAMMAR TEACHER - Testing with Examples\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚ùì Student Question 1: s·ª≠ d·ª•ng th√¨ hi·ªán t·∫°i ho√†n th√†nh nh∆∞ th·∫ø n√†o?\n",
      "======================================================================\n",
      "\n",
      "üìö Retrieved Context (first 500 chars):\n",
      "16  \n",
      "‚Ä¢ Khi nh·∫•n m·∫°nh ƒë·∫øn t·ª´ng th√†nh vi√™n, d√πng ƒë·ªông t·ª´ s·ªë nhi·ªÅu.  o V√≠ d·ª•: The team are arguing among themselves. L·ªói th∆∞·ªùng g·∫∑p: Kh√¥ng nh·∫•t qu√°n trong vi·ªác s·ª≠ d·ª•ng ƒë·ªông t·ª´ Sai: The staff is preparing their reports. S·ª≠a: The staff are preparing their reports. (N·∫øu nh·∫•n m·∫°nh t·ª´ng ng∆∞·ªùi trong nh√≥m) 2.2.9. Ch·ªß ng·ªØ l√† ph√¢n s·ªë ho·∫∑c ph·∫ßn trƒÉm Quy t·∫Øc: Khi ch·ªß ng·ªØ l√† ph√¢n s·ªë ho·∫∑c ph·∫ßn trƒÉm, ƒë·ªông t·ª´ ph√π h·ª£p v·ªõi danh t·ª´ theo sau \"of\". V√≠ d·ª•: ‚Ä¢ Fifty percent of the work is completed. ‚Ä¢ Two-thirds of the s...\n",
      "\n",
      "üéì Teacher Response:\n",
      "----------------------------------------------------------------------\n",
      "1. Th√¨ hi·ªán t·∫°i ho√†n th√†nh (Present Perfect) kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p chi ti·∫øt trong ng·ªØ c·∫£nh ƒë√£ truy xu·∫•t. Tuy nhi√™n, c√≥ m·ªôt l·ªói li√™n quan ƒë·∫øn vi·ªác s·ª≠ d·ª•ng hi·ªán t·∫°i ho√†n th√†nh thay v√¨ hi·ªán t·∫°i ho√†n th√†nh ti·∫øp di·ªÖn: \"Sai: I have lived here for five years (nh·∫•n m·∫°nh qu√° tr√¨nh). S·ª≠a: I have been living here for five years.\"\n",
      "\n",
      "2. Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ th√¨ hi·ªán t·∫°i ho√†n th√†nh:\n",
      "   - √ù nghƒ©a: Th√¨ hi·ªán t·∫°i ho√†n th√†nh di·ªÖn t·∫£ m·ªôt h√†nh ƒë·ªông ƒë√£ x·∫£y ra trong qu√° kh·ª© nh∆∞ng c√≥ li√™n quan ƒë·∫øn hi·ªán t·∫°i ho·∫∑c v·∫´n c√≤n ti·∫øp di·ªÖn ƒë·∫øn hi·ªán t·∫°i.\n",
      "   - C√°ch d√πng: Th√¨ n√†y th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ n√≥i v·ªÅ kinh nghi·ªám, nh·ªØng thay ƒë·ªïi, ho·∫∑c nh·ªØng h√†nh ƒë·ªông ƒë√£ x·∫£y ra nhi·ªÅu l·∫ßn trong qu√° kh·ª© v√† c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn hi·ªán t·∫°i.\n",
      "   - C·∫•u tr√∫c: S + have/has + V3 (past participle).\n",
      "\n",
      "3. V√≠ d·ª•: Kh√¥ng c√≥ v√≠ d·ª• c·ª• th·ªÉ v·ªÅ th√¨ hi·ªán t·∫°i ho√†n th√†nh trong ng·ªØ c·∫£nh ƒë√£ truy xu·∫•t.\n",
      "\n",
      "4. Gi·∫£i th√≠ch ƒë∆°n gi·∫£n: Th√¨ hi·ªán t·∫°i ho√†n th√†nh ƒë∆∞·ª£c d√πng ƒë·ªÉ n√≥i v·ªÅ nh·ªØng vi·ªác ƒë√£ x·∫£y ra trong qu√° kh·ª© nh∆∞ng v·∫´n c√≥ li√™n quan ƒë·∫øn hi·ªán t·∫°i. V√≠ d·ª•, n·∫øu b·∫°n ƒë√£ s·ªëng ·ªü ƒë√¢u ƒë√≥ trong m·ªôt th·ªùi gian d√†i v√† v·∫´n c√≤n s·ªëng ·ªü ƒë√≥, b·∫°n c√≥ th·ªÉ d√πng th√¨ n√†y ƒë·ªÉ n√≥i v·ªÅ ƒëi·ªÅu ƒë√≥.\n",
      "\n",
      "5. Th√¨ hi·ªán t·∫°i ho√†n th√†nh kh√¥ng ƒë∆∞·ª£c gi·∫£i th√≠ch chi ti·∫øt trong ng·ªØ c·∫£nh ƒë√£ truy xu·∫•t.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "‚ùì Student Question 2: th√¨ t∆∞∆°ng lai ƒë∆°n l√† g√¨ ? \n",
      "======================================================================\n",
      "\n",
      "üìö Retrieved Context (first 500 chars):\n",
      "38  \n",
      "Gi·∫£i th√≠ch: Ti·ªÅn t·ªë ph·ªß ƒë·ªãnh c·ªßa \"responsible\" l√† \"ir-\". L·ªói 3: Nh·∫ßm l·∫´n gi·ªØa t·ª´ g·ªëc v√† t·ª´ bi·∫øn th·ªÉ Sai: They made a great improve. S·ª≠a: They made a great improvement. Gi·∫£i th√≠ch: \"Improvement\" l√† danh t·ª´, \"improve\" l√† ƒë·ªông t·ª´.  4.4. Chi·∫øn l∆∞·ª£c nh·∫≠n di·ªán v√† s·ª≠a l·ªói 4.4.1. X√°c ƒë·ªãnh ch·ª©c nƒÉng t·ª´ trong c√¢u ‚Ä¢ H·ªèi xem t·ª´ ƒë√≥ ƒëang l√†m g√¨ trong c√¢u: l√†m ch·ªß ng·ªØ, ƒë·ªông t·ª´, t√≠nh t·ª´, tr·∫°ng t·ª´... ‚Ä¢ D·ª±a v√†o v·ªã tr√≠ c·ªßa t·ª´ ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i t·ª´ c·∫ßn thi·∫øt. 4.4.2. Nh·∫≠n di·ªán h·∫≠u t·ªë v√† ti·ªÅn t·ªë ‚Ä¢ H·∫≠u t·ªë th∆∞·ªùng ch...\n",
      "\n",
      "üéì Teacher Response:\n",
      "----------------------------------------------------------------------\n",
      "1. Th√¨ t∆∞∆°ng lai ƒë∆°n (Future Simple) l√† m·ªôt th√¨ trong ti·∫øng Anh d√πng ƒë·ªÉ di·ªÖn t·∫£ d·ª± ƒëo√°n ho·∫∑c quy·∫øt ƒë·ªãnh ƒë∆∞·ª£c ƒë∆∞a ra t·∫°i th·ªùi ƒëi·ªÉm n√≥i.\n",
      "\n",
      "2. C·∫•u tr√∫c c·ªßa th√¨ t∆∞∆°ng lai ƒë∆°n l√†: \n",
      "   - S + will + V-nguy√™n th·ªÉ\n",
      "\n",
      "3. V√≠ d·ª•: \n",
      "   - She will travel to Japan next month.\n",
      "\n",
      "4. Th√¨ t∆∞∆°ng lai ƒë∆°n d√πng ƒë·ªÉ n√≥i v·ªÅ nh·ªØng vi·ªác m√† ch√∫ng ta nghƒ© s·∫Ω x·∫£y ra trong t∆∞∆°ng lai ho·∫∑c nh·ªØng quy·∫øt ƒë·ªãnh m√† ch√∫ng ta ƒë∆∞a ra ngay l√∫c n√≥i. ƒê·ªÉ t·∫°o c√¢u ·ªü th√¨ n√†y, ch√∫ng ta d√πng \"will\" r·ªìi th√™m ƒë·ªông t·ª´ nguy√™n th·ªÉ ph√≠a sau. V√≠ d·ª•, n·∫øu b·∫°n mu·ªën n√≥i ai ƒë√≥ s·∫Ω ƒëi du l·ªãch Nh·∫≠t B·∫£n v√†o th√°ng sau, b·∫°n c√≥ th·ªÉ n√≥i: \"She will travel to Japan next month.\"\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "‚úÖ RAG Grammar Teacher is ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Create RAG chain with OpenAI and custom prompt ---\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"‚ö†Ô∏è OPENAI_API_KEY not found in environment. Please set it before running this cell.\")\n",
    "else:\n",
    "    # Initialize OpenAI LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.3,\n",
    "        api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    # Custom prompt template for the grammar teacher role\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\"You are an English grammar teacher. \"\n",
    "            \"A Vietnamese student has asked you a question about grammar.\\n\\n\"\n",
    "            \"RETRIEVED CONTEXT:\\n{context}\\n\\n\"\n",
    "            \"STUDENT QUESTION:\\n{question}\\n\\n\"\n",
    "            \"Please answer using Vietnamese language following these steps:\\n\"\n",
    "            \"1. Carefully read the CONTEXT retrieved from the database. Only use information that appears in the CONTEXT.\"\n",
    "            \"2. Give a short and clear explanation of the grammar point the student is asking about. Explain the meaning, Explain the usage, Explain the structure (if included in the context).\\n\"\n",
    "            \"3. Provide an example (use examples from the context if available). If the retrieved context contains examples, include at least one example verbatim and label it exactly as 'V√≠ d·ª•:'.\\n\"\n",
    "            \"4. Re-explain the concept using simpler Vietnamese so that a language learner can understand it easily.\\n\"\n",
    "            \"5. If the concept does not exist in the retrieved context, tell me honestly.\\n\\n\"\n",
    "            \"Your response:\"\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create a retrieval function that gets context from vector store\n",
    "    def get_context(query, k=5):\n",
    "        \"\"\"Retrieve top-k chunks from FAISS and format as context.\n",
    "        If none of the top-k contain example markers (e.g. 'v√≠ d·ª•', 'ƒë√°p √°n'),\n",
    "        try a lightweight fallback by scanning the saved JSONL chunks for a chunk\n",
    "        containing example markers and append it to the context.\n",
    "        \"\"\"\n",
    "        results = vector_store.similarity_search(query, k=5)\n",
    "        texts = [doc.page_content for doc in results]\n",
    "\n",
    "        # Check for example markers in retrieved chunks\n",
    "        markers = ['v√≠ d·ª•', 'ƒë√°p √°n', 'v√≠-d·ª•', 'v√≠du', 'example', 'ans:']\n",
    "        def has_example(text):\n",
    "            t = text.lower()\n",
    "            return any(m in t for m in markers)\n",
    "\n",
    "        contains_example = any(has_example(t) for t in texts)\n",
    "\n",
    "        # Fallback: scan saved JSONL for a chunk with an example marker and append it\n",
    "        if not contains_example:\n",
    "            try:\n",
    "                import json\n",
    "                with open('INTENSIVE_GRAMMAR_chunks.jsonl', 'r', encoding='utf-8') as f:\n",
    "                    for ln in f:\n",
    "                        obj = json.loads(ln)\n",
    "                        txt = obj.get('text', '').lower()\n",
    "                        if any(m in txt for m in markers):\n",
    "                            # append this chunk's original text to the context and stop\n",
    "                            texts.append(obj.get('text', ''))\n",
    "                            contains_example = True\n",
    "                            break\n",
    "            except FileNotFoundError:\n",
    "                # Saved JSONL not found; skip fallback\n",
    "                pass\n",
    "\n",
    "        context = \"\\n\\n---\\n\\n\".join(texts)\n",
    "        return context\n",
    "    \n",
    "    # Strengthen prompt: require verbatim example labeled 'V√≠ d·ª•:' if available\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=(\n",
    "            \"You are an English grammar teacher. \"\n",
    "            \"A Vietnamese student has asked you a question about grammar.\\n\\n\"\n",
    "            \"RETRIEVED CONTEXT:\\n{context}\\n\\n\"\n",
    "            \"STUDENT QUESTION:\\n{question}\\n\\n\"\n",
    "            \"Please answer using Vietnamese language following these steps:\\n\"\n",
    "            \"1. Carefully read the CONTEXT retrieved from the database. Only use information that appears in the CONTEXT.\"\n",
    "            \"2. Give a short and clear explanation of the grammar point the student is asking about. Explain the meaning, Explain the usage, Explain the structure (if included in the context).\\n\"\n",
    "            \"3. Provide an example (use examples from the context if available). If the retrieved context contains examples, include at least one example verbatim and label it exactly as 'V√≠ d·ª•:'.\\n\"\n",
    "            \"4. Re-explain the concept using simpler Vietnamese so that a language learner can understand it easily.\\n\"\n",
    "            \"5. If the concept does not exist in the retrieved context, tell me honestly.\\n\\n\"\n",
    "            \"Your response:\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Build RAG chain using LangChain's pipe operator\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": lambda x: get_context(x[\"question\"]),\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt_template\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    # Diagnostic helper to print top-k results and whether they contain example markers\n",
    "    def diagnose_question(question, k=5):\n",
    "        print(f\"\\n[DIAGNOSTIC] Running similarity_search_with_score for: '{question}' (k={k})\")\n",
    "        try:\n",
    "            results_with_scores = vector_store.similarity_search_with_score(question, k=k)\n",
    "        except Exception as e:\n",
    "            print(\"Error running similarity_search_with_score:\", e)\n",
    "            return\n",
    "\n",
    "        markers = ['v√≠ d·ª•', 'ƒë√°p √°n', 'v√≠-d·ª•', 'v√≠du', 'example', 'ans:']\n",
    "        for i, (doc, score) in enumerate(results_with_scores, start=1):\n",
    "            text = getattr(doc, 'page_content', str(doc))\n",
    "            preview = text[:800].replace('\\n', ' ')\n",
    "            lowered = text.lower()\n",
    "            contains_example = any(m in lowered for m in markers)\n",
    "            print(f\"--- Result {i} (score={score:.4f}) contains_example={contains_example} ---\")\n",
    "            print(\"metadata:\", getattr(doc, 'metadata', {}))\n",
    "            print(preview)\n",
    "            print('\\n')\n",
    "\n",
    "        # Quick JSONL search for a common tense phrase to show where examples live\n",
    "        try:\n",
    "            import json\n",
    "            pattern = 'hi·ªán t·∫°i ho√†n th√†nh'\n",
    "            found = 0\n",
    "            with open('INTENSIVE_GRAMMAR_chunks.jsonl', 'r', encoding='utf-8') as f:\n",
    "                for ln in f:\n",
    "                    obj = json.loads(ln)\n",
    "                    txt = obj.get('text', '').lower()\n",
    "                    if pattern in txt:\n",
    "                        found += 1\n",
    "                        print('\\n--- Found chunk containing pattern (preview) ---')\n",
    "                        print(obj.get('text', '')[:1200].replace('\\n', ' '))\n",
    "                        break\n",
    "            if found == 0:\n",
    "                print(f\"No saved chunk containing '{pattern}' found in INTENSIVE_GRAMMAR_chunks.jsonl\")\n",
    "        except FileNotFoundError:\n",
    "            print('INTENSIVE_GRAMMAR_chunks.jsonl not found; run the chunking cell first')\n",
    "    \n",
    "    print(\"‚úÖ RAG chain initialized!\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INTERACTIVE GRAMMAR TEACHER - Testing with Examples\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test queries to demonstrate the RAG chain\n",
    "    test_questions = [\n",
    "        \"s·ª≠ d·ª•ng th√¨ hi·ªán t·∫°i ho√†n th√†nh nh∆∞ th·∫ø n√†o?\",\n",
    "        \"th√¨ t∆∞∆°ng lai ƒë∆°n l√† g√¨ ? \"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚ùì Student Question {i}: {question}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Retrieve context\n",
    "            context = get_context(question)\n",
    "            print(f\"\\nüìö Retrieved Context (first 500 chars):\")\n",
    "            print(context[:500] + \"...\" if len(context) > 500 else context)\n",
    "            \n",
    "            # Get response from RAG chain\n",
    "            print(f\"\\nüéì Teacher Response:\")\n",
    "            print(\"-\" * 70)\n",
    "            # Pass a mapping matching the chain's input contract: {'question': ...}\n",
    "            response = rag_chain.invoke({\"question\": question})\n",
    "            # response may be a LangChain message object, a dict, or other runnable result.\n",
    "            # Extract text content safely.\n",
    "            if hasattr(response, 'content'):\n",
    "                answer_text = response.content\n",
    "            elif isinstance(response, dict) and 'content' in response:\n",
    "                answer_text = response['content']\n",
    "            else:\n",
    "                answer_text = str(response)\n",
    "            print(answer_text)\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing question: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ RAG Grammar Teacher is ready!\")\n",
    "    print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
